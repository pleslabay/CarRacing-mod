{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tensorflow version warnings\n",
    "import os\n",
    "# https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import warnings\n",
    "# https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines.common.policies import CnnPolicy #, MlpPolicy, CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv \n",
    "from stable_baselines import PPO2\n",
    "\n",
    "from stable_baselines.common.evaluation import evaluate_policy as test\n",
    "from stable_baselines.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## if you wish to set which cores to use\n",
    "affinity_mask = {4, 5, 7} \n",
    "#affinity_mask = {6, 7, 9} \n",
    "#affinity_mask = {0, 1, 3} \n",
    "#affinity_mask = {2, 3, 5} \n",
    "affinity_mask = {0, 2, 4, 6} \n",
    "\n",
    "pid = 0\n",
    "os.sched_setaffinity(pid, affinity_mask) \n",
    "print(\"CPU affinity mask is modified to %s for process id 0\" % affinity_mask) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## DEFAULT 'CarRacing-v3' environment values\n",
    "\n",
    "# continuos action = (steering_angle, throttle, brake)\n",
    "ACT = [[0, 0, 0], [-0.4, 0, 0], [0.4, 0, 0], [0, 0.6, 0], [0, 0, 0.8]]\n",
    "# discrete actions: center_steering and no gas/brake, steer left, steer right, accel, brake  \n",
    "#     --> actually a good choice, because car_dynamics softens the action's diff for gas and steering\n",
    "\n",
    "##REWARDS \n",
    "# reward given each step: step taken, distance to centerline, normalized speed [0-1], normalized steer angle [0-1]\n",
    "# reward given on new tile touched: %proportional of advance, %advance/steps_taken\n",
    "# reward given at episode end: all tiles touched (track finished), patience or off-raod exceeded, out of bounds, max_steps exceeded\n",
    "# reward for obstacles:  obstacle hit (each step), obstacle collided (episode end)\n",
    "GYM_REWARD = [ -0.1, 0.0, 0.0, 0.0,   10.0, 0.0,     0,  -0, -100, -0,     -0, -0 ]\n",
    "STD_REWARD = [ -0.1, 0.0, 0.0, 0.0,    1.0, 0.0,   100, -20, -100, -50,    -0, -0 ]\n",
    "CONT_REWARD =[-0.11, 0.1, 0.0, 0.0,    1.0, 0.0,   100, -20, -100, -50,    -5, -100 ]\n",
    "# see docu for RETURN computation details\n",
    "\n",
    "## DEFAULT Environment Parameters (not related to RL Algorithm!)\n",
    "game_color = 1           # State (frame) color option: 0 = RGB, 1 = Grayscale, 2 = Green only\n",
    "indicators = True        # show or not bottom Info Panel\n",
    "frames_per_state = 4     # stacked (rolling history) Frames on each state [1-inf], latest observation always on first Frame\n",
    "skip_frames = 3          # number of consecutive Frames to skip between history saves [0-4]\n",
    "discre = ACT             # Action discretization function, format [[steer0, throtle0, brake0], [steer1, ...], ...]. None for continuous\n",
    "\n",
    "use_track = 1            # number of times to use the same Track, [1-100]. More than 20 high risk of overfitting!!\n",
    "episodes_per_track = 1   # number of evenly distributed starting points on each track [1-20]. Every time you call reset(), the env automatically starts at the next point\n",
    "tr_complexity = 12       # generated Track geometric Complexity, [6-20]\n",
    "tr_width = 45            # relative Track Width, [30-50]\n",
    "patience = 2.0           # max time in secs without Progress, [0.5-20]\n",
    "off_track = 1.0          # max time in secs Driving on Grass, [0.0-5]\n",
    "f_reward = CONT_REWARD   # Reward Funtion coefficients, refer to Docu for details\n",
    "\n",
    "num_obstacles = 5        # Obstacle objects placed on track [0-10]\n",
    "end_on_contact = False   # Stop Episode on contact with obstacle, not recommended for starting-phase of training\n",
    "obst_location = 0        # array pre-setting obstacle Location, in %track. Negative value means tracks's left-hand side. 0 for random location\n",
    "oily_patch = False       # use all obstacles as Low-friction road (oily patch)\n",
    "verbose = 2      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose one agent, see Docu for description\n",
    "#agent='CarRacing-v0'\n",
    "#agent='CarRacing-v1'\n",
    "agent='CarRacing-v3'\n",
    "\n",
    "# Stop training when the model reaches the reward threshold\n",
    "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold = 170, verbose=1)\n",
    "\n",
    "seed = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMULATION param  \n",
    "## Changing these makes world models incompatible!!\n",
    "game_color = 2\n",
    "indicators = True\n",
    "fpst = 4\n",
    "skip = 3\n",
    "actions = [[0, 0, 0], [-0.4, 0, 0], [0.4, 0, 0], [0, 0.6, 0], [0, 0, 0.8]]  #this is ACT\n",
    "\n",
    "obst_loc = [6, -12, 25, -50, 75, -37, 62, -87, 95, -29]  #track percentage, negative for obstacle to the left-hand side\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading drive_pretained model\n",
    "\n",
    "import pickle\n",
    "root = 'ppo_cnn_gym-mod_'\n",
    "file = root+'c{:d}_f{:d}_s{:d}_{}_a{:d}'.format(game_color,fpst,skip,indicators,len(actions))\n",
    "\n",
    "model = PPO2.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This model param\n",
    "use = 6       # number of times to use same track [1,100]\n",
    "ept = 10      # different starting points on same track [1,20]\n",
    "patience = 1.0\n",
    "track_complexity = 12\n",
    "#REWARD2 = [-0.05, 0.1, 0.0, 0.0,   2.0, 0.0,   100, -20, -100, -50,   -5, -100]\n",
    "\n",
    "if agent=='CarRacing-v3': \n",
    "    env = gym.make(agent, seed=seed, \n",
    "        game_color=game_color,\n",
    "        indicators=indicators,\n",
    "        frames_per_state=fpst,\n",
    "        skip_frames=skip,   \n",
    "#        discre=actions,          #passing custom actions\n",
    "        use_track = use,       \n",
    "        episodes_per_track = ept,  \n",
    "        tr_complexity = track_complexity, \n",
    "        tr_width = 45,\n",
    "        patience = patience,\n",
    "        off_track = patience,\n",
    "        end_on_contact = True,     #learning to avoid obstacles the-hard-way\n",
    "        oily_patch = False,\n",
    "        num_obstacles = 5,         #some obstacles\n",
    "        obst_location = obst_loc,  #passing fixed obstacle location\n",
    "#        f_reward = REWARD2,        #passing a custom reward function\n",
    "        verbose = 2 )            \n",
    "else: \n",
    "    env = gym.make(agent)\n",
    "\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training on obstacles\n",
    "model.set_env(env)\n",
    "batch_size = 256\n",
    "updates = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = updates*batch_size, log_interval=1) #, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save last updated model\n",
    "\n",
    "file = root+'c{:d}_f{:d}_s{:d}_{}_a{:d}__u{:d}_e{:d}_p{}_bs{:d}'.format(\n",
    "    game_color,fpst,skip,indicators,len(actions),use,ept,patience,batch_size)\n",
    "\n",
    "model.save(file, cloudpickle=True)\n",
    "param_list=model.get_parameter_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## This model param #2\n",
    "use = 6       # number of times to use same track [1,100]\n",
    "ept = 10      # different starting points on same track [1,20]\n",
    "patience = 1.0\n",
    "track_complexity = 12\n",
    "#REWARD2 = [-0.05, 0.1, 0.0, 0.0,   2.0, 0.0,   100, -20, -100, -50,   -5, -100]\n",
    "seed = 25000\n",
    "\n",
    "if agent=='CarRacing-v3': \n",
    "    env2 = gym.make(agent, seed=seed, \n",
    "        game_color=game_color,\n",
    "        indicators=indicators,\n",
    "        frames_per_state=fpst,\n",
    "        skip_frames=skip,   \n",
    "#        discre=actions,          #passing custom actions\n",
    "        use_track = use,       \n",
    "        episodes_per_track = ept,  \n",
    "        tr_complexity = track_complexity, \n",
    "        tr_width = 45,\n",
    "        patience = patience,\n",
    "        off_track = patience,\n",
    "        end_on_contact = False,    # CHANGED \n",
    "        oily_patch = False,\n",
    "        num_obstacles = 5,         #some obstacles\n",
    "        obst_location = 0,         #using random obstacle location\n",
    "#        f_reward = REWARD2,        #passing a custom reward function\n",
    "        verbose = 3 )            \n",
    "else: \n",
    "    env2 = gym.make(agent)\n",
    "\n",
    "env2 = DummyVecEnv([lambda: env2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training on obstacles\n",
    "model.set_env(env2)\n",
    "#batch_size = 384\n",
    "updates = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate evaluation env\n",
    "test_freq = 100      #policy updates until evaluation\n",
    "test_episodes_per_track = 5   #number of starting points on test_track\n",
    "eval_log = './evals/'\n",
    "\n",
    "env_test = gym.make(agent, seed=int(3.14*seed), \n",
    "        game_color=game_color,\n",
    "        indicators=indicators,\n",
    "        frames_per_state=fpst,\n",
    "        skip_frames=skip,   \n",
    "#        discre=actions,            #passing custom actions\n",
    "        use_track = 1,           #change test track after 1 ept round\n",
    "        episodes_per_track = test_episodes_per_track,  \n",
    "        tr_complexity = 12,      #test on a medium complexity track\n",
    "        tr_width = 45,\n",
    "        patience = 2.0,\n",
    "        off_track = 2.0,\n",
    "        end_on_contact = False,\n",
    "        oily_patch = False,\n",
    "        num_obstacles = 5,\n",
    "        obst_location = obst_loc)  #passing fixed obstacle location\n",
    "\n",
    "env_test = DummyVecEnv([lambda: env_test])\n",
    "\n",
    "eval_callback = EvalCallback(env_test, callback_on_new_best=callback_on_best,  #None,\n",
    "                             n_eval_episodes=test_episodes_per_track*3, eval_freq=test_freq*batch_size,\n",
    "                             best_model_save_path=eval_log, log_path=eval_log, deterministic=True, \n",
    "                             render = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps = updates*batch_size, log_interval=1, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save last updated model\n",
    "\n",
    "#file = root+'c{:d}_f{:d}_s{:d}_{}_a{:d}__u{:d}_e{:d}_p{}_bs{:d}'.format(\n",
    "#    game_color,fpst,skip,indicators,len(actions),use,ept,patience,batch_size)\n",
    "\n",
    "model.save(file+'_II', cloudpickle=True)\n",
    "param_list=model.get_parameter_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.close()\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enjoy last trained policy\n",
    "\n",
    "if agent=='CarRacing-v3':  #create an independent test environment, almost everything in std/random definition\n",
    "    env3 = gym.make(agent, seed=None, \n",
    "        game_color=game_color,\n",
    "        indicators = True,\n",
    "        frames_per_state=fpst,\n",
    "        skip_frames=skip,   \n",
    "#        discre=actions,\n",
    "        use_track = 2,       \n",
    "        episodes_per_track = 1,  \n",
    "        patience = 5.0,\n",
    "        off_track = 3.0    )\n",
    "else:\n",
    "    env3 = gym.make(agent)\n",
    "\n",
    "env3 = DummyVecEnv([lambda: env3])\n",
    "obs = env3.reset()\n",
    "print(obs.shape)        \n",
    "\n",
    "done = False\n",
    "pasos = 0\n",
    "_states=None\n",
    "\n",
    "while not done: # and pasos<1500:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env3.step(action)\n",
    "    env3.render()\n",
    "    pasos+=1\n",
    "    \n",
    "env3.close()\n",
    "print()\n",
    "print(reward, done, pasos) #, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Enjoy best eval_policy\n",
    "\n",
    "obs = env3.reset()\n",
    "print(obs.shape)        \n",
    "\n",
    "## Load bestmodel from eval\n",
    "#if not isinstance(model_test, PPO2):\n",
    "model_test = PPO2.load(eval_log+'best_model', env3)\n",
    "\n",
    "done = False\n",
    "pasos = 0\n",
    "_states=None\n",
    "\n",
    "while not done: # and pasos<1500:\n",
    "    action, _states = model_test.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env3.step(action)\n",
    "    env3.render()\n",
    "    pasos+=1\n",
    "    \n",
    "env3.close()\n",
    "print()\n",
    "print(reward, done, pasos)\n",
    "print(action, _states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.save(file+'_evalbest', cloudpickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(action, _states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
